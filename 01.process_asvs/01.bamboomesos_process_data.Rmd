---
title: "Bamboo mesocosms experiment"
date: "`r format(Sys.time(), '%d %B %Y')`"
output:
 rmarkdown::html_document:
    theme: cerulean
    toc: yes
    toc_float: yes
    highlight: haddock
    number_sections: true
editor_options: 
  chunk_output_type: console
---

# Raw .fastq processing

Files are in their own folders.. copying them to one folder

```{bash, eval=F}
mkdir raw_fastqs

for dir in *L001*/
do
  cd $dir 
  cp *.fastq.gz ../raw_fastqs
  cd ..
done
```

# Setup things

## Libs

```{r}
#installing/loading packages:
#if (!requireNamespace("BiocManager", quietly = TRUE))
#  install.packages("BiocManager")
#BiocManager::install("dada2") 
library(dada2); packageVersion("dada2")
#Version 1.16.0
library(ShortRead)
#packageVersion("ShortRead")
#1.46.0
library(Biostrings)
#packageVersion("Biostrings")
#2.56.0

setwd("~/Desktop/Raw_REUSUM23_16slibrary/fastqs_altogether")

path <- "~/Desktop/Raw_REUSUM23_16slibrary/fastqs_altogether" # CHANGE ME to the directory containing the fastq files after unzipping.
```

## Functions

```{r}
allOrients <- function(primer) {
    # Create all orientations of the input sequence
    require(Biostrings)
    dna <- DNAString(primer)  # The Biostrings works w/ DNAString objects rather than character vectors
    orients <- c(Forward = dna, Complement = Biostrings::complement(dna), Reverse = Biostrings::reverse(dna),
        RevComp = Biostrings::reverseComplement(dna))
    return(sapply(orients, toString))  # Convert back to character vector
}

primerHits <- function(primer, fn) {
    # Counts number of reads in which the primer is found
    nhits <- vcountPattern(primer, sread(readFastq(fn)), fixed = FALSE)
    return(sum(nhits > 0))
}
```

# Checking for primers

[Tutorial here](https://benjjneb.github.io/dada2/ITS_workflow.html)

```{r, eval=F}
path <- "~/Desktop/Raw_REUSUM23_16slibrary/fastqs_altogether"  ## CHANGE ME to the directory containing the fastq files.
list.files(path)

fnFs <- sort(list.files(path, pattern = "_R1_001.fastq.gz", full.names = TRUE))
fnRs <- sort(list.files(path, pattern = "_R2_001.fastq.gz", full.names = TRUE))

FWD <- "GTGYCAGCMGCCGCGGTAA"  ## CHANGE ME to your forward primer sequence
REV <- "GGACTACNVGGGTWTCTAAT"  ## CHANGE ME...

FWD.orients <- allOrients(FWD)
REV.orients <- allOrients(REV)
FWD.orients

fnFs.filtN <- file.path(path, "filtN", basename(fnFs)) # Put N-filtered files in filtN/ subdirectory
fnRs.filtN <- file.path(path, "filtN", basename(fnRs))
filterAndTrim(fnFs, fnFs.filtN, fnRs, fnRs.filtN, maxN = 0, multithread = TRUE)

rbind(FWD.ForwardReads = sapply(FWD.orients, primerHits, fn = fnFs.filtN[[1]]), FWD.ReverseReads = sapply(FWD.orients,
    primerHits, fn = fnRs.filtN[[1]]), REV.ForwardReads = sapply(REV.orients, primerHits,
    fn = fnFs.filtN[[1]]), REV.ReverseReads = sapply(REV.orients, primerHits, fn = fnRs.filtN[[1]]))
```

All in revcomp at the end of the read, so not worried about trimming them

# Call ASVs with DADA2

Excellent walkthrough (v1.16) is [here](https://benjjneb.github.io/dada2/tutorial.html)

```{r, eval=F}
path <- "~/Desktop/Raw_REUSUM23_16slibrary/fastqs_altogether"  ## CHANGE ME to the directory containing the fastq files.
list.files(path)

fnFs <- sort(list.files(path, pattern = "_R1_001.fastq.gz", full.names = TRUE))
fnRs <- sort(list.files(path, pattern = "_R2_001.fastq.gz", full.names = TRUE))

sample.names <- sapply(strsplit(basename(fnFs), "_"), `[`, 1)
sample.names
```

## Raw visuals{.tabset}

### Forward

```{r, eval=F}
#First, lets look at quality profile of R1 reads
plotQualityProfile(fnFs[c(1,2,3,4)])
plotQualityProfile(fnFs[c(88,89,90,91)])
#looks great up to 200, maybe 190 to be safe
```

### Reverse

```{r, eval=F}
#Then look at quality profile of R2 reads
plotQualityProfile(fnRs[c(1,2,3,4)])
plotQualityProfile(fnRs[c(88,89,90,91)])
#190ish again
```

## Make ASV table

```{r, eval=F}
# Make directory and filenames for the filtered fastqs
filt_path <- file.path(path, "trimmed")
if(!file_test("-d", filt_path)) dir.create(filt_path)
filtFs <- file.path(filt_path, paste0(sample.names, "_F_filt.fastq.gz"))
filtRs <- file.path(filt_path, paste0(sample.names, "_R_filt.fastq.gz"))

#changing a bit from default settings - maxEE=1 (1 max expected error, more conservative), truncating length at 175 bp for both forward & reverse [leaves ~50bp overlap], added "trimleft" to cut off primers [18 for forward, 20 for reverse]
out <- filterAndTrim(fnFs, filtFs, fnRs, filtRs, 
                     truncLen=c(220,190), #leaves ~50bp overlap
                     maxN=0, #DADA does not allow Ns
                     maxEE=c(2,2), #allow 2 errors
                     truncQ=2, 
                     #trimLeft=c(18,20), #N nucleotides to remove from the start of each read
                     rm.phix=TRUE, #remove reads matching phiX genome
                     matchIDs=TRUE, #enforce matching between id-line sequence identifiers of F and R reads
                     compress=TRUE, 
                     verbose=TRUE,
                     multithread=TRUE) # On Windows set multithread=FALSE

head(out)
tail(out)

errF <- learnErrors(filtFs, multithread=TRUE)
errR <- learnErrors(filtRs, multithread=TRUE)

#sanity check: visualize estimated error rates
#error rates should decline with increasing qual score
#red line is based on definition of quality score alone
#black line is estimated error rate after convergence
#dots are observed error rate for each quality score

plotErrors(errF, nominalQ=TRUE) 
plotErrors(errR, nominalQ=TRUE) 

dadaFs <- dada(filtFs, err=errF, multithread=TRUE)
dadaRs <- dada(filtRs, err=errR, multithread=TRUE)

#now, look at the dada class objects by sample
#will tell how many 'real' variants in unique input seqs
#By default, the dada function processes each sample independently, but pooled processing is available with pool=TRUE and that may give better results for low sampling depths at the cost of increased computation time. See our discussion about pooling samples for sample inference. 
dadaFs[[1]]
dadaRs[[1]]

### Merge paired reads

#To further cull spurious sequence variants
#Merge the denoised forward and reverse reads
#Paired reads that do not exactly overlap are removed
mergers <- mergePairs(dadaFs, filtFs, dadaRs, filtRs, verbose=TRUE)
# Inspect the merger data.frame from the first sample
head(mergers[[1]])

#summary((mergers[[1]]))

#We now have a data.frame for each sample with the merged $sequence, its $abundance, and the indices of the merged $forward and $reverse denoised sequences. Paired reads that did not exactly overlap were removed by mergePairs.

seqtab <- makeSequenceTable(mergers)
dim(seqtab)
#172 samples, 10140 ASVs

# Inspect distribution of sequence lengths
table(nchar(getSequences(seqtab)))
#all in 252-254, will add one extra

##save raw output, I doubt I'll change it but just in case
#saveRDS(seqtab,file="bamboomesos_seqtab.rds")

##visual
#plot(table(nchar(getSequences(seqtab)))) 

#The sequence table is a matrix with rows corresponding to (and named by) the samples, and 
#columns corresponding to (and named by) the sequence variants. 
#Sequences that are much longer or shorter than expected may be the result of non-specific priming, and may be worth removing

seqtab2 <- seqtab[,nchar(colnames(seqtab)) %in% seq(251,255)] #again, being fairly conservative wrt length

#The core dada method removes substitution and indel errors, but chimeras remain. 
#Fortunately, the accuracy of the sequences after denoising makes identifying chimeras easier 
#than it is when dealing with fuzzy OTUs: all sequences which can be exactly reconstructed as 
#a bimera (two-parent chimera) from more abundant sequences.

seqtab.nochim <- removeBimeraDenovo(seqtab2, method="consensus", multithread=TRUE, verbose=TRUE)
dim(seqtab.nochim)
#Identified 7814 bimeras out of 10098 input sequences.

sum(seqtab.nochim)/sum(seqtab2)
#90.6% reads

# Track Read Stats #
getN <- function(x) sum(getUniques(x))
track <- cbind(out, sapply(dadaFs, getN), sapply(mergers, getN), rowSums(seqtab2), rowSums(seqtab.nochim))
colnames(track) <- c("input", "filtered", "denoised", "merged", "tabled", "nonchim")
rownames(track) <- sample.names
head(track)
tail(track)

##where to save
setwd("~/nicolagk@hawaii.edu - Google Drive/My Drive/Bamboo_mesos/Bamboo_mesos/01.process_asvs")
#write.csv(track,file="./raw_data/bamboomesos_readstats.csv",row.names=TRUE,quote=FALSE)
#saveRDS(seqtab.nochim,file="./raw_data/bamboomesos_seqtab.nochim.rds")
seqtab.nochim <- readRDS("./raw_data/bamboomesos_seqtab.nochim.rds")
##2284 ASVs, 172 samples
```

# Renaming ASVs

Renaming with ASV IDs which will be easier to work with

```{r}
library("dada2")

asv.ids <- paste0("ASV",sprintf('%0.4d', 1:length(colnames(seqtab.nochim))))
#making output fasta file - did once then skipping
#uniquesToFasta(seqtab.nochim, fout="bamboomesos_seqtab.nochim.fasta", ids = asv.ids, mode = "w", width = 20000)

dim(seqtab.nochim)
##172 samples, 2284 ASVs

##making a copy before I overwrite with new ids
seqtab.nochim.ids <- seqtab.nochim
colnames(seqtab.nochim.ids) <- asv.ids

##also the sample names still have extra info I don't want
rownames(seqtab.nochim.ids) <- sub("_F_filt.fastq.gz","",rownames(seqtab.nochim.ids))

#saveRDS(seqtab.nochim.ids,"bamboomesos_seqtab.nochim.ids.rds")
```

# LULU clustering

First produce a match list using blastn in Terminal. Made a copy of the fasta and doing this in folder 'lulu_things' because quite a bit of files are created

```{bash, eval=F}
#First produce a blastdatabase with the OTUs
makeblastdb -in bamboomesos_seqtab.nochim_copy.fasta -parse_seqids -dbtype nucl

# Then blast the OTUs against the database to produce the match list 
blastn -db bamboomesos_seqtab.nochim_copy.fasta -outfmt '6 qseqid sseqid pident' -out match_list.txt -qcov_hsp_perc 90 -perc_identity 84 -query bamboomesos_seqtab.nochim_copy.fasta
```

Back in R

```{r}
#first, read in ASV table
#install.packages("remotes")
library("remotes")
#install_github("https://github.com/tobiasgf/lulu.git")
library("lulu")

seqtab.nochim.ids <- readRDS("bamboomesos_seqtab.nochim.ids.rds")

#And match list
matchList <- read.table("./lulu_things/match_list.txt")
head(matchList)

#Reformat ASV table to desired LULU format
ASVs <- data.frame(t(seqtab.nochim.ids),check.names=FALSE)

#Now, run the LULU curation
##attempting to match CMAIKI steps
curated_result <- lulu(ASVs,matchList, minimum_ratio_type = "min", minimum_ratio = 1, minimum_match = 97, minimum_relative_cooccurence = 1)

summary(curated_result) #327 discarded otus

#Pull out the curated OTU list, re-transpose
seqtab.nochim.lulu <- data.frame(t(curated_result$curated_table),check.names=FALSE)

##fix the lulu order - didn't end up needing
#seqtab.nochim.lulu <- select(seqtab.nochim.lulu1, num_range("ASV", 0:2284))

#Continue on to your favorite analysis
#saveRDS(seqtab.nochim.lulu,file="bamboomesos_seqtab.nochim.lulu.rds")
```

# Assign taxonomy

## Setup

```{r}
setwd("~/nicolagk@hawaii.edu - Google Drive/My Drive/Bamboo_mesos/Bamboo_mesos/01.process_asvs")

seqtab.lulu <- readRDS("bamboomesos_seqtab.nochim.lulu.rds")
#172 samples, 1957 ASVs

# seqtab.lulu50 <- seqtab.lulu[,1:50]
# sum(seqtab.lulu50)

##otus to keep in fasta for taxonomic assignment
asvids.lulu <- colnames(seqtab.lulu)
#write.table(asvids.lulu, file="asvids.lulu.txt", append = FALSE, sep = "/n", row.names = FALSE, col.names = FALSE,quote=FALSE)
```

Subset fasta

```{bash, eval=F}
awk -F'>' 'NR==FNR{ids[$0]; next} NF>1{f=($2 in ids)} f' asvids.lulu.txt bamboomesos_seqtab.nochim.fasta >> bamboomesos_seqtab.nochim.lulu.fasta
```

## Begin assignment

Doing dada2's version on the cluster because it's slower than DECIPHER's version but I can't figure out if DECIPHER has an ID training set that is SILVA version 138.1

```{bash, eval=F}
srun -I30 -p sandbox -c 1 --mem=6g -t 60 --pty /bin/bash
module load lang/R
R

##if installing dada2:
#Selected CRAN 67 [Michigan]
#if (!requireNamespace("BiocManager", quietly = TRUE))
#    install.packages("BiocManager")
#BiocManager::install("dada2")

##alternatively, just checking that dada2 is installed:
library("dada2")
q() #to quit R

module avail lang/R/
##get the full loaded name for next time 
#lang/R/4.2.1-foss-2022a
```

SLURM job for taxonomy

```{bash}
nano assign_tax.slurm
```

Text within:

```
#!/bin/bash
#SBATCH --job-name=assign_tax
#SBATCH --partition=shared
##3 day max run time for public partitions, except 4 hour max runtime for the sandbox partition
#SBATCH --time=0-24:00:00
#SBATCH --cpus-per-task=1
#SBATCH --mem=20g
#SBATCH --error=%A.err
#SBATCH --output=%A.out ##%A = filled with job id

module load lang/R/4.2.1-foss-2022a
Rscript assign_tax.R
```

Quit & save. Then make the R script. Note: I have the fasta file but want to make it a .csv so I can read it in easily. There's a lovely resource here: https://cdcgov.github.io/CSV2FASTA/

```{bash}
nano assign_tax.R
```

The text in the assign_tax.R script:

```
library("dada2")

seqs <- read.csv("bamboomesos_seqtab.nochim.lulu.fasta.csv") #fasta file converted to .csv

head(seqs)

taxa <- assignTaxonomy(seqs$seq, "silva_nr99_v138.1_wSpecies_train_set.fa.gz", tryRC=TRUE,verbose=TRUE,minBoot=60)

saveRDS(taxa,file="bamboomesos_taxav138.1.rds")
```

Save & quit, submit:

```{bash, eval=F}
sbatch assign_tax.slurm
```

## Process taxa object back in R

```{r}
library("stringr")

taxa <- readRDS("bamboomesos_taxav138.1.rds")
##with ASV ids
taxa.ids <- read.csv("bamboomesos_seqtab.nochim.lulu.fasta.csv")
head(taxa.ids$seq == row.names(taxa))
tail(taxa.ids$seq == row.names(taxa))

##make a copy before editing
tax <- data.frame(taxa)
tax$id <- taxa.ids$id
tax$sequence <- row.names(tax)

tax.clean <- data.frame(row.names = row.names(tax),
                        Kingdom = str_replace(tax[,1], "D_0__",""),
                        Phylum = str_replace(tax[,2], "D_1__",""),
                        Class = str_replace(tax[,3], "D_2__",""),
                        Order = str_replace(tax[,4], "D_3__",""),
                        Family = str_replace(tax[,5], "D_4__",""),
                        Genus = str_replace(tax[,6], "D_5__",""),
                        Species = str_replace(tax[,7], "D_6__",""),
                        stringsAsFactors = FALSE)
tax.clean[is.na(tax.clean)] <- ""

for (i in 1:7){ tax.clean[,i] <- as.character(tax.clean[,i])}
####### Fill holes in the tax table
tax.clean[is.na(tax.clean)] <- ""
for (i in 1:nrow(tax.clean)){
  if (tax.clean[i,2] == ""){
    kingdom <- paste("Kingdom_", tax.clean[i,1], sep = "")
    tax.clean[i, 2:7] <- kingdom
  } else if (tax.clean[i,3] == ""){
    phylum <- paste("Phylum_", tax.clean[i,2], sep = "")
    tax.clean[i, 3:7] <- phylum
  } else if (tax.clean[i,4] == ""){
    class <- paste("Class_", tax.clean[i,3], sep = "")
    tax.clean[i, 4:7] <- class
  } else if (tax.clean[i,5] == ""){
    order <- paste("Order_", tax.clean[i,4], sep = "")
    tax.clean[i, 5:7] <- order
  } else if (tax.clean[i,6] == ""){
    family <- paste("Family_", tax.clean[i,5], sep = "")
    tax.clean[i, 6:7] <- family
  } else if (tax.clean[i,7] == ""){
    tax.clean$Species[i] <- paste("Genus",tax.clean$Genus[i], sep = "_")
  }
}

tax.clean[,8:9] <- tax[,8:9]
##checking merging the info back together went okay
row.names(tax.clean)==tax.clean[,"sequence"]
tail(row.names(tax.clean)==tax.clean[,"sequence"])

row.names(tax.clean) <- tax.clean$id

#write.csv(tax.clean,file="bamboomesos_taxa.csv")
```

# Cleaning ASVs & make phyloseq object

## Setup

```{r}
library('phyloseq')
library('ggplot2')
library('Rmisc')
library(cowplot)
library(ShortRead)
#BiocManager::install("decontam")
library(decontam)
library("dplyr")

setwd("~/nicolagk@hawaii.edu - Google Drive/My Drive/Bamboo_mesos/Bamboo_mesos/01.process_asvs")
```

## Removing singletons

```{r}
##counts table
seqtab.lulu <- readRDS("bamboomesos_seqtab.nochim.lulu.rds")

##raw reads for trimming levels below
seqtab.lulu.sums <- seqtab.lulu
seqtab.lulu.sums$sums <- rowSums(seqtab.lulu.sums)
sample.sums.df <- seqtab.lulu.sums[,1958, drop=FALSE]

##remove singletons
seqtab.lulu.pa <- seqtab.lulu
seqtab.lulu.pa[seqtab.lulu.pa > 0] <- 1 
seqtab.lulu.nosing <- seqtab.lulu.pa[,colSums(seqtab.lulu.pa)!=1]
##leaves 792 ASVs
colnames.keep <- c(colnames(seqtab.lulu.nosing))

##subset the ones to keep in the original table
seqtab.lulu.keep <- seqtab.lulu[,colnames(seqtab.lulu) %in% c(colnames.keep)] 

#write.csv(seqtab.lulu.keep,file="bamboomesos_seqtab.nochim.lulu.nosing.csv")
```

## Trimming <0.1% abundance per sample

Decided on this threshold by examining the results of the positive control reads

```{r}
seqtab.trimmed <- seqtab.lulu.keep
##checking the sample sums line up with the current otu table
row.names(seqtab.trimmed)==row.names(sample.sums.df)

for (i in 1:nrow(seqtab.trimmed)) {
  #row_sum <- sum(seqtab.trimmed[i,])
  threshold <- sample.sums.df[i,] * 0.001001
  #print(threshold)
  seqtab.trimmed[i,seqtab.trimmed[i,] <= threshold] <- 0
}

tail(sort(colSums(seqtab.trimmed),decreasing=T),500)
##remove 0s
seqtab.trimmed2 <- seqtab.trimmed[,colSums(seqtab.trimmed)>0]
##540 ASVs
tail(sort(colSums(seqtab.trimmed2),decreasing=T),500)

```

## Remove ASVs supported by less than 0.001% of total reads - skipping

```{r}
#sum(rowSums(seqtab.lulu)) #13546765
#sum(rowSums(seqtab.lulu))*0.00001 #135

#seqtab.trimmed.135 <- seqtab.trimmed[colSums(seqtab.trimmed)>135]
# tail(sort(colSums(seqtab.lulu),decreasing=T)) #before
# tail(sort(colSums(seqtab.trimmed),decreasing=T)) #after
# sum(rowSums(seqtab.trimmed.135)) #13245039
##leaves 476 ASVs
```

## Make phyloseq object

```{r}
taxa <- read.csv("bamboomesos_taxa.csv",row.names=1)

samdf <- read.csv("../bamboomesos_metadata_plusmgs.csv")
##the underscore gets changed somewhere for no reason
#samdf$Short_label <- gsub("PCR_NEG_","PCR-NEG-",samdf$Short_label)
#samdf$Short_label <- gsub("PCR_POS_","PCR-POS-",samdf$Short_label)
rownames(samdf) <- samdf$Short_label

#phyloseq object with new taxa ids
##otu table that's trimmed
ps1 <- phyloseq(otu_table(seqtab.trimmed2, taxa_are_rows=FALSE), 
               sample_data(samdf), 
               tax_table(as.matrix(taxa)))

ps1 

# ##otu table that's not trimmed, for stuff below
# ps1 <- phyloseq(otu_table(seqtab.lulu.keep,taxa_are_rows=FALSE),
#                sample_data(samdf),
#                tax_table(as.matrix(taxa)))
# 
# ps1

##zeroes left from trimming stuff?
ps <- prune_taxa(taxa_sums(ps1)!=0,ps1)
ps
# phyloseq-class experiment-level object
# otu_table()   OTU Table:         [ 540 taxa and 172 samples ]
# sample_data() Sample Data:       [ 172 samples by 14 sample variables ]
# tax_table()   Taxonomy Table:    [ 540 taxa by 9 taxonomic ranks ]

ps.pos1 <- subset_samples(ps,Sample_type=="Positive")
ps.pos <- prune_taxa(taxa_sums(ps.pos1)!=0,ps.pos1)
ps.pos@otu_table
```

## Remove chloroplasts etc. 

```{r}
tax.pre <- data.frame(ps@tax_table)

#ps.mito <- subset_taxa(ps, Family=="Mitochondria")
#ps.mito #0 taxa to remove
ps.chlor <- subset_taxa(ps, Order=="Chloroplast")
ps.chlor #1 taxa to remove
ps.notbact <- subset_taxa(ps, Kingdom!="Bacteria")
ps.notbact #1 taxa to remove

ps.nomito <- subset_taxa(ps, Family!="Mitochondria")
ps.nomito 
ps.nochlor <- subset_taxa(ps.nomito, Order!="Chloroplast")
ps.nochlor
ps.clean <- subset_taxa(ps.nochlor, Kingdom=="Bacteria")
ps.clean #538 taxa

#just archaea
ps.arch <- subset_taxa(ps, Kingdom=="Archaea")
ps.arch #1 taxa
```

## Remove contamination from negative controls

```{r, eval=F}
df <- as.data.frame(sample_data(ps.clean)) # Put sample_data into a ggplot-friendly data.frame
df$LibrarySize <- sample_sums(ps.clean)
df <- df[order(df$LibrarySize),]
df$Index <- seq(nrow(df))
ggplot(data=df, aes(x=Index, y=LibrarySize, color=Mesocosm_type)) + geom_point()

sample_data(ps.clean)$lib_size_clean <- sample_sums(ps.clean)
sample_data(ps.clean)$is.neg <- sample_data(ps.clean)$Mesocosm_type == "Negative"
contamdf.prev <- isContaminant(ps.clean, neg="is.neg",threshold=0.5)
table(contamdf.prev$contaminant)
# FALSE  TRUE 
#   514    24 

# Make phyloseq object of presence-absence in negative controls and true samples
ps.pa <- transform_sample_counts(ps.clean, function(abund) 1*(abund>0))
ps.pa.neg <- prune_samples(sample_data(ps.pa)$Mesocosm_type == "Negative", ps.pa)
ps.pa.pos <- prune_samples(sample_data(ps.pa)$Mesocosm_type != "Negative", ps.pa)
# Make data.frame of prevalence in positive and negative samples
df.pa <- data.frame(pa.pos=taxa_sums(ps.pa.pos), pa.neg=taxa_sums(ps.pa.neg),
                    contaminant=contamdf.prev$contaminant)
ggplot(data=df.pa, aes(x=pa.neg, y=pa.pos, color=contaminant)) + geom_point() +
  xlab("Prevalence (Negative Controls)") + ylab("Prevalence (True Samples)")

#remove contaminants from ps.clean
ps.clean.decontam1 <- prune_taxa(!contamdf.prev$contaminant,ps.clean)
#also remove negative controls, don't need them anymore I think
#ps.clean.decontam2 <- subset_samples(ps.clean.decontam1,(Mesocosm_type!="Negative"))
#remove any taxa equal to 0
ps.clean.decontam2 <- prune_taxa(taxa_sums(ps.clean.decontam1)!=0,ps.clean.decontam1)
ps.clean.decontam2
#any samples equal to 0? 
ps.clean.decontam <- prune_samples(sample_sums(ps.clean.decontam2)!=0,ps.clean.decontam2)
ps.clean.decontam 
# phyloseq-class experiment-level object
# otu_table()   OTU Table:          [ 514 taxa and 169 samples ]:
# sample_data() Sample Data:        [ 169 samples by 24 sample variables ]:
# tax_table()   Taxonomy Table:     [ 514 taxa by 10 taxonomic ranks ]:
# taxa are columns

tail(taxa_sums(ps.clean.decontam))
tail(sort(sample_sums(ps.clean.decontam),decreasing=T))

#saveRDS(ps.clean.decontam,file="bamboomesos_ps.lulu.clean.decontam.rds")
#saveRDS(ps.clean.decontam,file="bamboomesos_ps.lulu.clean.notrim.decontam.rds")

##checking positive control real quick
ps.pos <- subset_samples(ps.clean.decontam,Mesocosm_type=="Positive")
ps.pos.no0 <- prune_taxa(taxa_sums(ps.pos)!=0,ps.pos)
ps.pos.no0
ps.pos.no0@otu_table

##prior to negative control removal
ps.notclean.pos <- subset_samples(ps,Mesocosm_type=="Positive")
ps.notclean.pos.no0 <- prune_taxa(taxa_sums(ps.notclean.pos)!=0,ps.notclean.pos)
ps.notclean.pos.no0
##46 is being thrown out as neg contamination, but it's in the positive control
seqtab.lulu %>%
  as.data.frame() %>%
  dplyr::select(ASV0046)
##looks good to take out :) 
```

## Saving extra files

```{r, eval=F}
library('phyloseq')

setwd("~/nicolagk@hawaii.edu - Google Drive/My Drive/Bamboo_mesos/Bamboo_mesos/01.process_asvs")

ps.all <- readRDS("bamboomesos_ps.lulu.clean.decontam.rds")
metadat <- data.frame(ps.all@sam_data)
write.csv(metadat,file="./bamboomesos_processed_data/bamboomesos_metadata.csv")
countstab <- data.frame(ps.all@otu_table)
write.csv(countstab,file="./bamboomesos_processed_data/bamboomesos_countstab.csv")
taxatab <- data.frame(ps.all@tax_table)
write.csv(taxatab,file="./bamboomesos_processed_data/bamboomesos_taxtab.csv")


```

## ASVs left after quality filtering to subset from fasta - didn't do yet

```{r}
ps.exp1 <- subset_samples(ps.clean,Mesocosm_type=="Experiment")
ps.exp1
ps.exp2 <- subset_samples(ps.exp1,Exp_day!=20)
ps.exp2
ps.exp3 <- subset_samples(ps.exp2,Microbe_treatment!="AHK")
ps.exp3
ps.exp <- prune_taxa(taxa_sums(ps.exp3)!=0,ps.exp3)
ps.exp
otu.exp <- data.frame(ps.exp@otu_table)

otu.exp.list <- colnames(otu.exp)
#write.table(otu.exp.list, file="otus.experiment.clean.txt", append = FALSE, sep = "/n", row.names = FALSE, col.names = FALSE,quote=FALSE)

tax.exp <- data.frame(ps.exp@tax_table)
#write.csv(tax.exp,file="otus.experiment.taxa.csv")
```

# Rarefy

Doesn't make sense to rarefy a bunch of these samples

```{r, eval=F}
ps.clean.decontam <- readRDS("bamboomesos_ps.lulu.clean.decontam.rds")
ps.clean.decontam #514 taxa, 169 samples

sample_data(ps.clean.decontam)$lib_size_clean <- sample_sums(ps.clean.decontam)

samdf.clean <- data.frame(ps.clean.decontam@sam_data)
#write.csv(samdf.clean,file="samdf.clean.csv")

samdf.low <- subset(samdf.clean,lib_size_clean<20000)
samdf.low

ggplot(data=samdf.low, aes(x=Longer_name, y=lib_size_clean, color=Mesocosm_type))+
  geom_point()+
  theme(axis.text.x=element_text(angle=45,hjust=1))
##all the lowest ones that aren't controls are in the E. coli only treatment which makes sense

df <- data.frame(ps.clean.decontam@otu_table) # Put sample_data into a ggplot-friendly data.frame
df$libsize <- rowSums(df)

df2 <- df[order(df$libsize),]

df2$Index <- seq(nrow(df2))
##all of them:
ggplot(data=df2, aes(x=Index, y=libsize))+
  geom_point()

head(df2$libsize,n=25) 
tail(df2$libsize,n=50)

##higher ones
ggplot(data=df2, aes(x=Index, y=libsize))+
  geom_point()+
  ylim(120000,270000)+
  xlim(130,173)
#there's definitely a natural break between 130950 and 144273
#10% of that would be 13,095
#or between 166362 and 195031
#so 10% of that would be 16k

##lower ones
ggplot(data=df2, aes(x=Index, y=libsize))+
  geom_point()+
  ylim(0,50000)+
  xlim(0,50)

set.seed(2939)

#ps.rare <- rarefy_even_depth(ps.clean.decontam,sample.size=10999,replace=FALSE)
#saveRDS(ps.rare,"bamboomesos_ps.lulu.clean.decontam.rare11k.rds")
##7 samples removed, 6 OTUs removed

ps.rare <- rarefy_even_depth(ps.clean.decontam,sample.size=6950,replace=FALSE)
#saveRDS(ps.rare,"bamboomesos_ps.lulu.clean.decontam.rare6.9k.rds")
##12 samples removed, 14 OTUs removed

##removing the same samples from the other ps object for continuity with comparisons
#ps.rare <- readRDS("bamboomesos_ps.lulu.clean.decontam.rare11k.rds")
#ps.rare #156 samples, 500 taxa

ps.clean.decontam

samdf.clean <- data.frame(ps.clean.decontam@sam_data)

#samdf.low <- subset(samdf.clean,lib_size_clean<10999)
samdf.low <- subset(samdf.clean,lib_size_clean<6950)

samdf.low
rownames(samdf.low)

otu.clean <- data.frame(ps.clean.decontam@otu_table)

otu.less <- subset(otu.clean,!rownames(otu.clean)%in% c(rownames(samdf.low)))
ps.clean.decontam.copy <- ps.clean.decontam
ps.clean.decontam.copy@otu_table <- otu_table(otu.less,taxa_are_rows = F)
ps.clean.decontam.copy

ps.less1 <- phyloseq(otu_table(otu.less, taxa_are_rows=FALSE), 
               sample_data(samdf.clean), 
               tax_table(as.matrix(taxa)))
ps.less1

ps.less <- prune_taxa(taxa_sums(ps.less1)>0,ps.less1)
ps.less #500 taxa, 157 samples

#saveRDS(ps.less,"bamboomesos_ps.lulu.clean.decontam.less.rds")
```


