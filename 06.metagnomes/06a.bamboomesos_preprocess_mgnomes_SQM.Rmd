---
title: "Bamboo mesocosms experiment - squeezemeta analysis"
date: "`r format(Sys.time(), '%d %B %Y')`"
output:
 rmarkdown::html_document:
    theme: cerulean
    toc: yes
    toc_float: yes
    highlight: haddock
    number_sections: true
editor_options: 
  chunk_output_type: console
---

# Per sample raw reads

```{bash, eval=F}
# Output file
output="mgs_raw_read_counts.tsv"
echo -e "Sample\tR1_reads\tR2_reads" > $output

# Get list of unique sample names (before _R1 or _R2)
for sample in $(ls *_R1_*.fastq.gz | sed 's/_R1_.*.fastq.gz//' | sort | uniq); do
    # Find R1 and R2 file paths
    R1_file=$(ls ${sample}_R1_*.fastq.gz)
    R2_file=$(ls ${sample}_R2_*.fastq.gz)

    # Count reads
    R1_count=$(zcat "$R1_file" | wc -l)
    R2_count=$(zcat "$R2_file" | wc -l)

    # Divide by 4 to get number of reads
    R1_reads=$((R1_count / 4))
    R2_reads=$((R2_count / 4))

    # Write to file
    echo -e "${sample}\t${R1_reads}\t${R2_reads}" >> $output
done
```

# Databases

Normally I would follow the [squeezemeta installation instructions](https://github.com/jtamames/SqueezeMeta), but Samira already downloaded the databases into a directory, so just making a copy of my own from there.

```{bash, eval=F}
cd /home/nicolagk/cmaiki_koastore/fatemi/DATABASES
```

Text within nk_copy.slurm:

```
#!/bin/bash
#SBATCH --job-name=nk_copy.slurm
#SBATCH --partition=shared
##3 day max run time for public partitions, except 4 hour max runtime for the sandbox partition
#SBATCH --time=0-72:00:00
#SBATCH --nodes=1
#SBATCH --cpus-per-task=10
#SBATCH --mem=100G
#SBATCH --error=%A.err
#SBATCH --output=%A.out ##%A = filled with job id
#SBATCH --mail-type=BEGIN,END,FAIL,REQUEUE,TIME_LIMIT_80
#SBATCH --mail-user=nicolagk@hawaii.edu

cp -r ./db /home/nicolagk/cmaiki_koastore/nicolagk/bamboomesos_metagnomes/squeeze
```
Save & exit, submit:

```{bash, eval=F}
sbatch nk_copy.slurm
```

Note: the actual bro5 metagenome results were also in the db folder, so those got copied over, will delete later

# Setting up squeeze meta

```{bash, eval=F}
##conda doesn't work unless I do this first:
srun -p sandbox -N 1 -c 1 --mem=6G -t 0-01:00:00 --pty /bin/bash

ml lang/Anaconda3
conda --version ##24.4.0

module list
##Currently Loaded Modules:
#  1) lang/Anaconda3/2024.02-1

##installation following instructions from the squeeze meta github:
conda install -n base conda-libmamba-solver
##output notes:
#Channels:
# - defaults
# - conda-forge
#Platform: linux-64

##ERROR:
#EnvironmentNotWritableError: The current user does not have write permissions to the target environment.
#  environment location: /opt/apps/software/lang/Anaconda3/2024.02-1
#  uid: 7033
#  gid: 7033

##^even though I got that error, the rest of this still worked:
conda config --set solver libmamba
##no errors on that one
conda create -n SqueezeMeta -c conda-forge -c bioconda -c fpusan squeezemeta=1.6 --no-channel-priority --override-channels
##says it will try to install a million packages
##not seeing any errors, but it does say this:
```

```
Krona installed.  You still need to manually update the taxonomy                
databases before Krona can generate taxonomic reports.  The update              
script is ktUpdateTaxonomy.sh.  The default location for storing                
taxonomic databases is /home/nicolagk/.conda/envs/SqueezeMeta/opt/krona/taxonomy
                                                                                
If you would like the taxonomic data stored elsewhere, simply replace           
this directory with a symlink.  For example:                                    
                                                                                
rm -rf /home/nicolagk/.conda/envs/SqueezeMeta/opt/krona/taxonomy                
mkdir /path/on/big/disk/taxonomy                                                
ln -s /path/on/big/disk/taxonomy /home/nicolagk/.conda/envs/SqueezeMeta/opt/krona/taxonomy                                                                      
ktUpdateTaxonomy.sh 

##end of text:                                                                               
# To activate this environment, use                                             
#                                                                               
#     $ conda activate SqueezeMeta
#
# To deactivate an active environment, use
#
#     $ conda deactivate
```

I didn't do any of the suggestions about Krona (yet). Script that checks whether things are working:

```{bash, eval=F}
##note: sometimes I have to run this interactively before running conda activate:
#source activate base

conda activate SqueezeMeta

##what version
SqueezeMeta.pl -v 
#1.6.5.post1, November 2024

##change where the databases are:
perl /home/nicolagk/.conda/envs/SqueezeMeta/SqueezeMeta/utils/install_utils/configure_nodb.pl /home/nicolagk/cmaiki_koastore/nicolagk/bamboomesos_metagnomes/squeeze/db

##test:
test_install.pl
```

Getting errors about "kmer-db", but according to [this issue page](https://github.com/jtamames/SqueezeMeta/issues/741) I likely don't need this program unless I was doing "seqmerge" mode (one of the assembly strategies)

# Running the pipeline

## Testing with just 3 samples

Text within sqm_bamboomesos_3samps.slurm

```
#!/bin/bash
#SBATCH --job-name=sqm_bamboomesos_3samps
#SBATCH --partition=exclusive-long
#SBATCH --time=7-00:00:00 ## time format is DD-HH:MM:SS
#SBATCH --nodes=4
#SBATCH --error=job%A.err ## %A - filled with jobid
#SBATCH --output=job%A.out ## %A - filled with jobid
#SBATCH --mail-type=BEGIN,END,FAIL,REQUEUE,TIME_LIMIT_80
#SBATCH --mail-user=nicolagk@hawaii.edu

ml lang/Anaconda3
source activate SqueezeMeta
SqueezeMeta.pl -m sequential -s /home/nicolagk/cmaiki_koastore/nicolagk/bamboomesos_metagnomes/3samps/file.names_3samps -f /home/nicolagk/cmaiki_koastore/nicolagk/bamboomesos_metagnomes/3samps --cleaning --nobins
```

Notes: -f is the directory where the .fastq.gz files are, -s is a text file that looks like this:

```
LB5	LB5_S15_L001_R1_001.fastq.gz	pair1
LB5	LB5_S15_L001_R2_001.fastq.gz	pair2
LL11	LL11_S6_L001_R1_001.fastq.gz	pair1
LL11	LL11_S6_L001_R2_001.fastq.gz	pair2
WB9	WB9_S10_L001_R1_001.fastq.gz	pair1
WB9	WB9_S10_L001_R2_001.fastq.gz	pair2
```

Another note: check that "--cleaning" is actually 2 short hyphens and not one long one, or it won't clean the reads

### Array option

```{bash, eval=F}
#!/bin/bash
#SBATCH --job-name=3samps_array
#SBATCH --partition=exclusive-long
#SBATCH --time=7-00:00:00  ## Format: DD-HH:MM:SS
#SBATCH --nodes=2
#SBATCH --array=1-3  ## Replace 'M' with the number of unique samples (not file lines!)
#SBATCH --error=job%A_%a.err  ## Job ID (%A), Task ID (%a)
#SBATCH --output=job%A_%a.out  ## Job ID (%A), Task ID (%a)
#SBATCH --mail-type=BEGIN,END,FAIL,REQUEUE,TIME_LIMIT_80
#SBATCH --mail-user=nicolagk@hawaii.edu

## Load environment
ml lang/Anaconda3
source activate SqueezeMeta

## Define paths
SAMPLE_LIST=/home/nicolagk/cmaiki_koastore/nicolagk/bamboomesos_metagnomes/3samps/sample_list.txt  ## Contains just unique sample names: LB5, LL11, WB9...
FULL_SAMPLE_FILE=/home/nicolagk/cmaiki_koastore/nicolagk/bamboomesos_metagnomes/3samps/file.names_3samps  ## Contains the paired-end file structure

## Get the sample name for this task
SAMPLE=$(sed -n "${SLURM_ARRAY_TASK_ID}p" "${SAMPLE_LIST}")

## Extract corresponding lines for this sample
grep -P "^${SAMPLE}\t" "${FULL_SAMPLE_FILE}" > "sample_${SAMPLE}.txt"

## Run SqueezeMeta on the extracted sample file
SqueezeMeta.pl -m sequential -s "sample_${SAMPLE}.txt" -f /home/nicolagk/cmaiki_koastore/nicolagk/bamboomesos_metagnomes/3samps --cleaning --nobins -t 20

```

## More than 3 samps

Above was just 3 samples, but I have many more. First, this script makes the formatted sample list file which is nice:

```{bash, eval=F}
for file in *_R1_001.fastq.gz; do base=$(echo "$file" | sed -E 's/_S[0-9]+_L001_R1_001.fastq.gz//'); echo -e "$base\t$file\tpair1"; echo -e "$base\t${file/_R1_/_R2_}\tpair2"; done > formatted_samples.tsv
```

Next I need the unique sample names for the array script:

```{bash, eval=F}
ls *.fastq.gz | cut -d'_' -f1 | sort -u > unique_samples.txt
```

Now the array script is the same as above, but there are 45 samples now

### Array option

```{bash, eval=F}
nano mostsamps_array.slurm
```

Text within:

```
#!/bin/bash
#SBATCH --job-name=mostsamps_array
#SBATCH --partition=exclusive
#SBATCH --time=3-00:00:00  ## Format: DD-HH:MM:SS
#SBATCH --nodes=5
#SBATCH --array=1-45%5  ## Replace 'M' with the number of unique samples (not file lines!)
#SBATCH --error=job%A_%a.err  ## Job ID (%A), Task ID (%a)
#SBATCH --output=job%A_%a.out  ## Job ID (%A), Task ID (%a)
#SBATCH --mail-type=BEGIN,END,FAIL,REQUEUE,TIME_LIMIT_80
#SBATCH --mail-user=nicolagk@hawaii.edu

## Load environment
ml lang/Anaconda3
source activate SqueezeMeta

## Define paths
SAMPLE_LIST=/home/nicolagk/cmaiki_koastore/nicolagk/bamboomesos_metagnomes/all_fastqs/unique_samples.txt  ## Contains just unique sample names: LB5, LL11, WB9...
FULL_SAMPLE_FILE=/home/nicolagk/cmaiki_koastore/nicolagk/bamboomesos_metagnomes/all_fastqs/formatted_samples.tsv  ## Contains the paired-end file structure

## Get the sample name for this task
SAMPLE=$(sed -n "${SLURM_ARRAY_TASK_ID}p" "${SAMPLE_LIST}")

## Extract corresponding lines for this sample
grep -P "^${SAMPLE}\t" "${FULL_SAMPLE_FILE}" > "sample_${SAMPLE}.txt"

## Run SqueezeMeta on the extracted sample file
SqueezeMeta.pl -m sequential -s "sample_${SAMPLE}.txt" -f /home/nicolagk/cmaiki_koastore/nicolagk/bamboomesos_metagnomes/all_fastqs --cleaning --nobins -t 20
```

# Packaging results for R

```{bash, eval=F}
srun -p sandbox -N 1 -c 1 --mem=6G -t 0-01:00:00 --pty /bin/bash

ml lang/Python
ml lang/Anaconda3
##note: sometimes I have to run this before running conda activate:
source activate base
conda activate SqueezeMeta
```

## Slurm script version

```{bash, eval=F}
nano sqm2zip.slurm
```

```
#!/bin/bash
#SBATCH --job-name=sqm2zip
#SBATCH --partition=shared
##3 day max run time for public partitions, except 4 hour max runtime for the sandbox partition
#SBATCH --time=0-72:00:00
#SBATCH --nodes=1
#SBATCH --cpus-per-task=10
#SBATCH --mem=100G
#SBATCH --error=%A.err
#SBATCH --output=%A.out ##%A = filled with job id
#SBATCH --mail-type=BEGIN,END,FAIL,REQUEUE,TIME_LIMIT_80
#SBATCH --mail-user=nicolagk@hawaii.edu

ml lang/Anaconda3
source activate SqueezeMeta

output_dir="./rready"

for dir in $(ls | grep -E '^(LB|LL|WB|WL)[0-9]+$'); do
    echo "Processing $dir"
    python /home/nicolagk/.conda/envs/SqueezeMeta/bin/sqm2zip.py $dir $output_dir
done
```

